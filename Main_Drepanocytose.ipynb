{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_Drepanocytose.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPf6G2H7PAtQtSDNVIyzTfl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed-Aziz-Mhadhbi/Drepanocytose/blob/main/Main_Drepanocytose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1mE6vXEggFa",
        "outputId": "d6978dd2-dc0c-48e8-caa0-e5bd55a0945f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Models'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 38 (delta 4), reused 31 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (38/38), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/InterxPim/Models.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Models/Drépanocytose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cs-tgOgjPkw",
        "outputId": "563a71b6-1d3f-4ac4-d9c3-423214405a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Models/Drépanocytose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirement.txt"
      ],
      "metadata": {
        "id": "xy-7EykBjjNG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12359bb2-1e77-4266-b93c-dad024bc6d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.4.3\n",
            "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting tensorflow-gpu==2.3.1\n",
            "  Downloading tensorflow_gpu-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 24 kB/s \n",
            "\u001b[?25hCollecting matplotlib==3.1.1\n",
            "  Downloading matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirement.txt (line 4)) (1.4.1)\n",
            "Collecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting numba==0.50.1\n",
            "  Downloading numba-0.50.1-cp37-cp37m-manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 41.3 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting scikit-image==0.17.2\n",
            "  Downloading scikit_image-0.17.2-cp37-cp37m-manylinux1_x86_64.whl (12.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.5 MB 25.7 MB/s \n",
            "\u001b[?25hCollecting opencv-python==4.1.0.25\n",
            "  Downloading opencv_python-4.1.0.25-cp37-cp37m-manylinux1_x86_64.whl (26.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6 MB 1.6 MB/s \n",
            "\u001b[?25hCollecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3->-r requirement.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3->-r requirement.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.1.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.15.0)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.44.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.2.0)\n",
            "Collecting h5py\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 34.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r requirement.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r requirement.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r requirement.txt (line 3)) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.1->-r requirement.txt (line 3)) (0.11.0)\n",
            "Collecting llvmlite<0.34,>=0.33.0.dev0\n",
            "  Downloading llvmlite-0.33.0-cp37-cp37m-manylinux1_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 491 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.50.1->-r requirement.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r requirement.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r requirement.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.17.2->-r requirement.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->-r requirement.txt (line 10)) (3.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->-r requirement.txt (line 10)) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->-r requirement.txt (line 10)) (4.63.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu==2.3.1->-r requirement.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->-r requirement.txt (line 10)) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9692 sha256=2b25a72f1a56036507f5b4f1e287c29257a526b9ce4d795eff083eba8fa40c03\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "Successfully built gdown\n",
            "Installing collected packages: numpy, tensorflow-estimator, matplotlib, llvmlite, h5py, gast, tensorflow-gpu, scikit-learn, scikit-image, opencv-python, numba, keras, gdown\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "tensorflow 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 2.4.3 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.0+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.1 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 gdown-3.12.2 h5py-2.10.0 keras-2.4.3 llvmlite-0.33.0 matplotlib-3.1.1 numba-0.50.1 numpy-1.18.5 opencv-python-4.1.0.25 scikit-image-0.17.2 scikit-learn-0.23.2 tensorflow-estimator-2.3.0 tensorflow-gpu-2.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras as K\n",
        "tf.__version__, K.__version__"
      ],
      "metadata": {
        "id": "9GbRf0uZkiSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import source.toolkit as tk\n",
        "import source.dojo_tools as dj\n",
        "\n",
        "from source.toolkit import list_channels\n",
        "from source.toolkit import list_channels_df\n",
        "\n",
        "# load main class object for monitoring blood cells\n",
        "from source.SickleML_Monitor import CountAdheredBloodCells\n",
        "\n",
        "# loading tools for extracting gdrive data\n",
        "import source.load_data_tools as loading_tools\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import gdown\n",
        "from natsort import natsorted"
      ],
      "metadata": {
        "id": "s9j10yRdklKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(f'./Phase2_Pretrained-models/Resnet50/', exist_ok = True)\n",
        "os.chdir(f'./Phase2_Pretrained-models/Resnet50/') \n",
        "\n",
        "loading_tools.resnet50_gdrive_()\n",
        "    \n",
        "os.chdir(f'../')\n",
        "os.chdir(f'../')\n",
        "\n",
        "\n",
        "os.makedirs(f'./Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/', exist_ok = True)\n",
        "os.chdir(f'./Phase1_Pretrained-models/ce-jaccard_encoder-decoder-net/') \n",
        "\n",
        "loading_tools.ce_jaccard_gdrive_()\n",
        "    \n",
        "os.chdir(f'../')\n",
        "os.chdir(f'../')\n"
      ],
      "metadata": {
        "id": "IxwHQdxtkqVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channel_path= './data/Manual-VS-AI/'\n",
        "os.makedirs(channel_path, exist_ok=True)\n",
        "os.chdir(channel_path)\n",
        "loading_tools.laminin_channel_gdrive_()\n",
        "os.chdir('../')\n",
        "os.chdir('../')\n",
        "channel_filenames  =  os.listdir(channel_path)"
      ],
      "metadata": {
        "id": "f3ZCQ-BHkxoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of channels: {len(channel_filenames)}')\n",
        "channel_filenames"
      ],
      "metadata": {
        "id": "h3PfGcp2k1mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ensembles(Phase1_path_model, Phase2_path_model):\n",
        "\n",
        "    Phase1_path = './Phase1_Pretrained-models/' + Phase1_path_model + '/'# folder for Phase I \n",
        "    Phase1_filenames = natsorted(os.listdir(Phase1_path))\n",
        "    Phase1_final_filenames = []\n",
        "    for ii in range(len(Phase1_filenames)):\n",
        "        if ii % 2 != 0:\n",
        "            Phase1_final_filenames.append(Phase1_filenames[ii].replace('.json', ''))\n",
        "\n",
        "    Phase2_path = './Phase2_Pretrained-models/' + Phase2_path_model + '/'# folder for Phase II\n",
        "    Phase2_filenames = natsorted(os.listdir(Phase2_path))\n",
        "    Phase2_final_filenames = []\n",
        "    for ii in range(len(Phase2_filenames)):\n",
        "        if ii % 2 != 0:\n",
        "            Phase2_final_filenames.append(Phase2_filenames[ii].replace('.json',''))\n",
        "\n",
        "\n",
        "    Phase1_ensemble = tk.load_zoo(Phase1_path, Phase1_final_filenames) # loading the Phase I ensemble (expect: 7)\n",
        "    Phase2_ensemble = tk.load_zoo(Phase2_path, Phase2_final_filenames) # loading the Phase I ensemble (expect: 5)\n",
        "    return Phase1_ensemble, Phase2_ensemble\n",
        "\n",
        "def create_final_df(counts, times):\n",
        "    counts_df = pd.DataFrame(counts)\n",
        "    counts_df.columns = ['filename', 'def-sRBC', 'nondef-sRBC', 'Other']\n",
        "    times_df = pd.DataFrame(times)\n",
        "    times_df.columns = ['time_secs']\n",
        "    final_df = pd.concat([counts_df, times_df], axis = 1)\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "3uHMyJfWk6wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "\n",
        "\n",
        "\n",
        "counts, times = [], [] \n",
        "count_container, time_container = [], []\n",
        "\n",
        "rbc_thres = [0.4]\n",
        "wbc_thres = [0.4]\n",
        "other_thres = [0.9]\n",
        "\n",
        "Phase1_names, Phase2_names = 'ce-jaccard_encoder-decoder-net', 'Resnet50'\n",
        "\n",
        "Phase1_ensemble, Phase2_ensemble = load_ensembles(Phase1_names, Phase2_names)\n",
        "counts, times = [], [] \n",
        "\n",
        "for index, filenames in enumerate(channel_filenames):\n",
        "    for rep in ((\".png\", \"\"), (\".jpg\", \"\")):\n",
        "        clean_filename = filenames.replace(*rep)\n",
        "    print('Analysis:', index, '| Channel:', clean_filename)\n",
        "    print('==================================================================')\n",
        "    channel = CountAdheredBloodCells(channel_path, filenames) # calling the class object\n",
        "    # calling the function to output cell counts\n",
        "    start = time.time()\n",
        "    sRBC, WBC, Others, img_container, sRBC_container, WBC_container, Other_container = channel.call_pipeline(Phase1_ensemble, Phase2_ensemble, rbc_thres, wbc_thres, other_thres)\n",
        "    end = time.time()\n",
        "    run_time = end-start\n",
        "            \n",
        "    times.append([run_time])\n",
        "    counts.append([filenames, sRBC, WBC, Others])\n",
        "            \n",
        "    final_df = create_final_df(counts,times)\n",
        "   # final_df.to_csv(f'./AI-vs-Human_counts/{Phase1_name}_{Phase2_name}.csv', index = False)\n"
      ],
      "metadata": {
        "id": "0AkZJCxtk-5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Here is the predicted counts ...\")\n",
        "final_df"
      ],
      "metadata": {
        "id": "Di7iOxIslGKj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}